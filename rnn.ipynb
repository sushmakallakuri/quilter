{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import keras\n",
    "import pandas as pd\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Input, concatenate, Dropout, Reshape\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "\n",
    "from keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"F:/talentsprintproject/quora-insincere-questions-classification\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TIME = 30\n",
    "VOCAB_SIZE = 20000\n",
    "QUES_CLEANING_PATTERN = re.compile(\"[\\s\\n\\r\\t.,:;\\-_\\'\\\"?!#&()\\/%\\[\\]\\{\\}\\<\\>\\\\$@\\!\\*\\+\\=]\")\n",
    "LSTM_DIM = 256\n",
    "LSTM_DIMS = [512,256]\n",
    "NUM_FILTERS = 5\n",
    "FILTER_LENGTHS = [1,2,3,4,5,8,10,15,20,25]\n",
    "DROPOUT = 0.4\n",
    "LEARNING_RATE = 0.005\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 2000\n",
    "NUM_UNDERSAMPLE = 3\n",
    "RUS_RATIO = 1.0/4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.array(train_data)\n",
    "test_data = np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1225312 80810 0.05135890827962472\n"
     ]
    }
   ],
   "source": [
    "ct_0 = 0\n",
    "c = 0\n",
    "for d in train_data:\n",
    "    if d[2] == 0:\n",
    "        ct_0 += 1\n",
    "    if len(QUES_CLEANING_PATTERN.split(d[1])) > MAX_TIME:\n",
    "        c+=1\n",
    "print(ct_0, len(train_data) - ct_0, c/len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN = int(len(train_data)*0.8)\n",
    "np.random.shuffle(train_data)\n",
    "train_x = train_data[:NUM_TRAIN,1]\n",
    "train_y = train_data[:NUM_TRAIN,2]\n",
    "val_x = train_data[NUM_TRAIN:,1]\n",
    "val_y = train_data[NUM_TRAIN:,2]\n",
    "del train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = test_data[:,1]\n",
    "test_ids = test_data[:,0]\n",
    "del test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = tokenizer.texts_to_sequences(train_x)\n",
    "val_x = tokenizer.texts_to_sequences(val_x)\n",
    "test_x = tokenizer.texts_to_sequences(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1044897,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddingMatrix(wordIndex):\n",
    "    \n",
    "    embeddingsIndex = {}\n",
    "    # Load the embedding vectors from ther GloVe file\n",
    "    with open(\"embeddings/glove.840B.300d/glove.840B.300d.txt\", encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split(' ')\n",
    "            word = values[0]\n",
    "            try:\n",
    "                embeddingVector = np.asarray(values[1:], dtype='float32')\n",
    "            except:\n",
    "                print(values)\n",
    "                break\n",
    "            embeddingsIndex[word] = embeddingVector\n",
    "\n",
    "    print('Found %s word vectors.' % len(embeddingsIndex))\n",
    "\n",
    "    # Minimum word index of any word is 1.\n",
    "    embeddingMatrix = np.zeros((len(wordIndex) + 1, 300))\n",
    "    for word, i in wordIndex.items():\n",
    "        embeddingVector = embeddingsIndex.get(word)\n",
    "        if embeddingVector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embeddingMatrix[i] = embeddingVector\n",
    "    del embeddingsIndex\n",
    "    return embeddingMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordIndex = tokenizer.word_index\n",
    "wI = {}\n",
    "for k,v in wordIndex.items():\n",
    "    if v < VOCAB_SIZE:\n",
    "        wI[k] = v\n",
    "wordIndex = wI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2196016 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddingMatrix = getEmbeddingMatrix(wordIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pad_sequences(train_x,maxlen=MAX_TIME,padding='post')\n",
    "val_x = pad_sequences(val_x,maxlen=MAX_TIME,padding='post')\n",
    "test_x = pad_sequences(test_x,maxlen=MAX_TIME,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = to_categorical(train_y)\n",
    "#train_y = train_y.astype('int')\n",
    "val_y = to_categorical(val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1044897, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel(embeddingMatrix):\n",
    "    embeddingLayer = Embedding(embeddingMatrix.shape[0],300,weights=[embeddingMatrix],\\\n",
    "                              input_length=MAX_TIME,trainable=False)\n",
    "   \n",
    "    sent_inp = Input(shape=(MAX_TIME,))\n",
    "    sent = embeddingLayer(sent_inp)\n",
    "    \n",
    "    _, h1, c1 = LSTM(LSTM_DIM,dropout=DROPOUT,return_state=True)(sent)\n",
    "    h1 = Reshape([MAX_TIME,LSTM_DIM])(concatenate([h1]*MAX_TIME,1))\n",
    "    lstm_inp = concatenate([sent,h1],axis=2)\n",
    "    _, h2, c2 = LSTM(LSTM_DIM*2,dropout=DROPOUT,return_state=True)(lstm_inp)\n",
    "    probs = Dense(64,activation='relu')(h2)\n",
    "    probs = Dense(2,activation='sigmoid')(probs)\n",
    "    model = Model(inputs=sent_inp,outputs=probs)\n",
    "    rmsprop = optimizers.rmsprop(lr=LEARNING_RATE)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=rmsprop,\n",
    "                 metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = buildModel(embeddingMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1044897 samples, validate on 261225 samples\n",
      "Epoch 1/10\n",
      "1044897/1044897 [==============================] - 6814s 7ms/step - loss: 0.1743 - acc: 0.9426 - val_loss: 0.1214 - val_acc: 0.9525\n",
      "Epoch 2/10\n",
      "1044897/1044897 [==============================] - 7843s 8ms/step - loss: 0.1220 - acc: 0.9523 - val_loss: 0.1285 - val_acc: 0.9530\n",
      "Epoch 3/10\n",
      " 150000/1044897 [===>..........................] - ETA: 1:27:20 - loss: 0.1159 - acc: 0.9546"
     ]
    }
   ],
   "source": [
    "model.fit(train_x,train_y,epochs=NUM_EPOCHS,batch_size=BATCH_SIZE,verbose=1,validation_data=(val_x,val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_x,batch_size=BATCH_SIZE)\n",
    "predictions = predictions.argmax(axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
